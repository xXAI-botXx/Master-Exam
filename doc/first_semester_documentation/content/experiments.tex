\chapter{Experiments}
\label{cha:experiments}

	% TO DO: 
	%     - Fill the gaps
	% test basesimulation as input
	% improve pipeline (unite and clean arg handling -> pydantic, 2 pipelines runable with same scripts, using the same self-generated satelite maps with multiple simulations settings, handling multi/parallel generation docker containers inclduing crashes with a script starting/finishing handler)
	% test WGAN-GP -> more stable GAN/Adversarial loss
	% test both improvements together
	% test new Architecture: Depth Anything (describe Architecture)
	% test new Architecture: HexaWaveNet -> best of 6 papers model
	% describe Architecture
	% needed more engineering to make it better
	% share some results and architecture versions
	% test new residual model design with Pix2Pix and DepthAnything and special loss
	% describe architecture (+ where you have this idea: a paper)
	% learning the complex only was a big challenge: extract complex only + special loss + depth anything adjustments
	% describe loss in detail -> weighted combined loss: weights are calculated with inverted histogram -> because a big imbalance + variance loss + range loss (min-max)


	\section{Input Abstraction}
	\label{sec:experiments-input-abstraction}
		% Description
		The Input Abstraction experiment investigates the impact of changing the input to the base propagation. The idea is that the translation is less complex if the input distribution already covers much of the output distribution. This is shown in figure \ref{fig:experiment_input}.
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.65\textwidth]{img/experiment_input.png}
			\caption[Experiment Idea for Input Abstraction]{Experiment Idea for Input Abstraction}
			\label{fig:experiment_input}
		\end{figure}
		\FloatBarrier
		There are 3 variations in which 2 pix2pix models are trained but one with the standard satelite image as input and the other with the base simulation as input. One variation where both models got trained 50 epochs, one with 100 epochs, and another where the output is changed from reflection to reflection and diffraction.
		
		% Implementation & Challenges
		In order to be able to conduct this experiment new datasets with the same building positions and in multiple variations (base and reflection) are needed. \\For that 4 datasets with 10k observations and random building positions and fewer amount of buildings were created.\\
		In that process, the pipeline of the Physgen (works with GPS locations) and the random buildings dataset creation got united by 2 running scripts which also allow guided parallel execution (handles crashes consistently) and the arguments (which came originally from different sources -> a file and python call) got also standardized and validated with a new Pydantic \cite{pydantic} argument handling which allows now partwise multiple settings for multiple dataset generation.\\
		% Results
		Table \ref{tab:performance_input_50} and \ref{tab:performance_input_100} show the mean average LoS and NLoS error and mean absolute percentage LoS and NLoS error with 50 and 100 epochs. And table \ref{tab:performance_input_ref_diff} shows the error comparison with reflection and diffraction as targets.
		
		\begin{table}[h!]
			\centering
			\begin{tabular}{|l|c|c|c|c|}
				\hline
				\textbf{Model} & \textbf{LoS MAE} & \textbf{NLoS MAE} & \textbf{LoS wMAPE} & \textbf{NLoS wMAPE} \\
				\hline
				Pix2Pix Standard Input & 5.55 & 9.20 & 26.71 & 20.85 \\
				Pix2Pix Base Input & \textbf{0.56} & \textbf{0.81} & \textbf{5.17} & \textbf{1.85} \\
				\hline
			\end{tabular}
			\caption{Performance comparison of Input Abstraction with 50 Epochs on 10k random generated buildings dataset.}
			\label{tab:performance_input_50}
		\end{table}
		
		\begin{table}[h!]
			\centering
			\begin{tabular}{|l|c|c|c|c|}
				\hline
				\textbf{Model} & \textbf{LoS MAE} & \textbf{NLoS MAE} & \textbf{LoS wMAPE} & \textbf{NLoS wMAPE} \\
				\hline
				Pix2Pix Standard Input & 2.16 & 2.03 & 10.62 & 4.75 \\
				Pix2Pix Base Input & \textbf{0.38} & \textbf{0.15} & \textbf{3.28} & \textbf{0.35} \\
				\hline
			\end{tabular}
			\caption{Performance comparison of Input Abstraction with 100 Epochs on 10k random generated buildings dataset.}
			\label{tab:performance_input_100}
		\end{table}
		
		\begin{table}[h!]
			\centering
			\begin{tabular}{|l|c|c|c|c|}
				\hline
				\textbf{Model} & \textbf{LoS MAE} & \textbf{NLoS MAE} & \textbf{LoS wMAPE} & \textbf{NLoS wMAPE} \\
				\hline
				Pix2Pix Standard Input & 2.44 & 3.33 & 12.96 & 7.46 \\
				Pix2Pix Base Input & \textbf{0.55} & \textbf{0.30} & \textbf{3.91} & \textbf{0.70} \\
				\hline
			\end{tabular}
			\caption{Performance comparison of Input Abstraction with reflection and diffraction as target on 10k random generated buildings dataset.}
			\label{tab:performance_input_ref_diff}
		\end{table}
		
		%\begin{figure}[H]
		%	\centering
		%	\includegraphics[width=0.8\textwidth]{img/experiment_input_res_50_epochs.png}
		%	\caption[Experiment Results Input Abstraction with 50 Epochs.]{Experiment Results Input Abstraction with 50 Epochs.}
		%	\label{fig:experiment_input_res_50}
		%\end{figure}
		%\FloatBarrier
		
		%\begin{figure}[H]
		%	\centering
		%	\includegraphics[width=0.8\textwidth]{img/experiment_input_res_100_epochs.png}
		%	\caption[Experiment Results Input Abstraction with 100 Epochs.]{Experiment Results Input Abstraction with 100 Epochs.}
		%	\label{fig:experiment_input_res_100}
		%\end{figure}
		%\FloatBarrier
		
		%\begin{figure}[H]
		%	\centering
		%	\includegraphics[width=0.8\textwidth]{img/experiment_input_res_50_epochs_ref_&_diffraction.png}
		%	\caption[Experiment Results Input Abstraction with reflection and diffraction as output.]{Experiment Results Input Abstraction with reflection and diffraction as output.}
		%	\label{fig:experiment_input_res_ref_dif}
		%\end{figure}
		%\FloatBarrier
		
		In all 3 experiments, the model with the base propagation as input is superior to the model with the standard input and is on average 3,6 times more precise.\\
		As an additional experiment, the 2 pix2pix models with base propagation as input (50 and 100 epochs) were compared to just the base propagation itself as output. It is expected that the base propagation is very close to a perfect loss in LoS but not precise in NLoS. Table \ref{tab:performance_input_base} shows that the model was indeed more precise in the NLoS area but not in the LoS. The expectations are met.
		\begin{table}[h!]
			\centering
			\begin{tabular}{|l|c|c|c|c|}
				\hline
				\textbf{Model} & \textbf{LoS MAE} & \textbf{NLoS MAE} & \textbf{LoS wMAPE} & \textbf{NLoS wMAPE} \\
				\hline
				Pix2Pix Base Input 50 & 0.56 & 0.81 & 5.17 & 1.85 \\
				Pix2Pix Base Input 100 & 0.38 & \textbf{0.15} & 3.28 & \textbf{0.35} \\
				Base Propagation & \textbf{0.32} & 2.88 & \textbf{0.70} & 7.70 \\
				\hline
			\end{tabular}
			\caption{Performance comparison of Input Abstraction with base propagation output on 10k reflection random generated buildings dataset.}
			\label{tab:performance_input_base}
		\end{table}
		\\
		%Hint: The results of this experiment can't be compared to the benchmark from \ref{tab:performance_base} due to the fact that this experiment uses another simpler dataset.
		
	
	\section{Stabilized Adversarial Loss}
	\label{sec:experiments-stabilized_adversarial_loss}
		% Description
		The impact of more stabilized training between discriminator and generator towards learning a complex distribution is researched in this experiment.\\
		\citetitle{noah_makow_wasserstein_2018} already tried out WGAN in an image-to-image translation task, but especially in an image generation task could suffer from gradient clipping which let this project led to experiment with the WGAN-GP \cite{gulrajani_improved_2017}.
		
		% Implementation & Challenges
		The original GAN loss uses binary cross entropy and looks like this:
		\begin{equation}
			Loss_G = -log( D( G(z) ) ) 
		\end{equation}
		
		\noindent where:
		\begin{itemize}[itemsep=1mm, parsep=0pt]
			\item $D$ is the discriminator/critic function
			\item $G$ is the generator function
			\item $z$ is the input image
		\end{itemize}
		
		\begin{equation}
			Loss_D = -log(D(x)) - log(1 - D(\tilde{x}))
		\end{equation}
		
		\noindent where:
		\begin{itemize}[itemsep=1mm, parsep=0pt]
			\item $x$ is a ground truth image
			\item $\tilde{x} = G(z)$ is a generated (fake) image
		\end{itemize}
		
		In comparison to that, the WGAN-GP uses the Wasserstein distance with an additional gradient penalty. The gradient penalty is needed to make sure the Lipschitz continuity condition is achieved. In other words, the gradient penalty makes sure that the gradients are not too low and not too high so that the generator can learn efficiently and stable from the discriminator's loss. To calculate this the gradients of an in-between representation of fake and real is calculated (the region where the generator and the discriminator interact at most with each other) and the length of the gradients is calculated. 
		
		\begin{equation}
			Loss_G = -D( G(z) )
		\end{equation}
		
		\noindent where:
		\begin{itemize}[itemsep=1mm, parsep=0pt]
			\item $D$ is the discriminator/critic function
			\item $G$ is the generator function
			\item $z$ is the input image
		\end{itemize}
		
		\begin{equation}
			Loss_D = D(\tilde{x}) - D(x) + \lambda \cdot \left( \| \nabla_{\hat{x}} D(\hat{x}) \|_2 - 1 \right)^2
		\end{equation}
		
		\noindent where:
		\begin{itemize}[itemsep=1mm, parsep=0pt]
			\item $x$ is a ground truth image
			\item $\tilde{x} = G(z)$ is a generated (fake) image
			\item $\hat{x} = \epsilon x + (1 - \epsilon) \tilde{x}$ is an interpolation between real and fake, with $\epsilon \sim \mathcal{U}(0,1)$
			\item $\lambda$ is a weight for the gradient penalty (usually $\lambda = 10$)
			\item $\nabla_{\hat{x}} D(\hat{x})$ is the gradient of $D$ with respect to $\hat{x}$
		\end{itemize}
		
		% Results
		The training loss got much smoother and convergence was achieved quicker with Wassterstein loss and Gradient Penalty compared to the binary cross entropy loss as figure \ref{fig:experiment_wgangp_process} shows.
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.9\textwidth]{img/experiment_wgangp_ L1_Loss.png}
			\caption[Training loss between a GAN pix2pix model and a WGAN-GP pi2pix model.]{Training loss between a GAN pix2pix model and a WGAN-GP pi2pix model.}
			\label{fig:experiment_wgangp_process}
		\end{figure}
		\FloatBarrier
		
		In terms of accuracy, the WGAN-GP model outperformed the GAN model in LoS and NLoS and is about 2 times more precise, as table \ref{tab:performance_wgangp} shows.
		
		\begin{table}[h!]
			\centering
			\begin{tabular}{|l|c|c|c|c|}
				\hline
				\textbf{Model} & \textbf{LoS MAE} & \textbf{NLoS MAE} & \textbf{LoS wMAPE} & \textbf{NLoS wMAPE} \\
				\hline
				Pix2Pix GAN & 4.10 & 7.52 & 28.92 & 17.59 \\
				Pix2Pix WGAN-GP & \textbf{1.42} & \textbf{4.22} & \textbf{17.27} & \textbf{10.97} \\
				\hline
			\end{tabular}
			\caption{Performance comparison of WAN and WGAN-GP model on 10k random generated buildings dataset.}
			\label{tab:performance_wgangp}
		\end{table}
		
	\section{Generator Architecture Exploration}
	\label{sec:experiments-generator_architecture_exploration}
		% Description
		As the paper from \citeauthor{achim_eckerle_evaluierung_2025} showed there are still architectures that are worth trying for paired image-to-image translation. This experiment tests 2 different models. One is the DepthAnything model \cite{yang2024depthv2} which normally is used for depth estimation but also just does image-to-image translation with a pre-trained DINOv2 Encoder \cite{oquab2024dinov2learningrobustvisual} and an adjusted DPT Decoder \cite{ranftl2021visiontransformersdenseprediction}.\\
		The other model is a self-invented model called Hexa-Wave-Net which combines 6 advances from research.
		
		% Implementation & Challenges
		For the DepthAnything model, a new Physgen dataset implementation was needed, a new adjusted loss, and the output activation function had to be adjusted. The loss was specialized for depth and so it got changed to a combined loss from l1 and sharp aware losses (which uses the deviation of the image). The ReLU activation functions were replaced by the LeakyReLU and the last activation was changed to a 0.0 to 1.0 clamping. These changes made the architecture able to predict sound propagation.\\
		5 variations were implemented for the Hexa-Wave-Net. The standard network with the architecture is shown in figure \ref{fig:experiment_architecture_hexa_wave_net}. So the 6 advances are:
		\begin{enumerate}[itemsep=1mm, parsep=0pt]
			\item CNN Encoder for local features (ConvNeXt \cite{liu2022convnet2020s})
			\item FNO (Fourier Neural Operator) Layer for global frequency feature extraction
			\item Latent Transformer Block as a kind of saliency filter
			\item SIREN Decoder for physical correct decoding
			\item Skip Connection for residual learning
			\item Half WGAN-GP - half sharp aware loss
		\end{enumerate}
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=1.0\textwidth]{img/experiment_architectures_hexa_wave_net.png}
			\caption[Hexa-Wave-Net architecture variation 1.]{Hexa-Wave-Net architecture variation 1.}
			\label{fig:experiment_architecture_hexa_wave_net}
		\end{figure}
		\FloatBarrier
		
		% add variations
		\clearpage
		Variation 2 removes the latent space transformer and changes the SIREN-Decoder to CNN and SIREN Decoder with a skip connection to the input image.\\
		Variation 3 is similar to variation 2 but combines the different decoder and the original image with an MLP Head (multilayer perceptron / fully connected layer).\\
		Variation 7 changes the Encoder-CNN from ConvNeXt to a simple CNN with skip connections, adds the Transformer Latent Space again, and uses also a simple CNN decoder (without SIREN-Decoder) and with the original image. At the end is again an MLP-Head.
		
		% Results
		The results from DepthAnything show that his absolute errors are a bit higher than the current SOTA while the relative errors are more than 3 times higher than the SOTA. It also seems like that DepthAnything made many errors in small value areas and so the relative values are much higher than the absolute errors. Table \ref{tab:experiment_architecture_results} contains all results.\\
		\\
		The results from the Hexa-Wave-Net variations differ much from each other. While variations 1 and 2 have massive errors, variations 3, 7, and 8 are much closer to the SOTA. Variation 7 interestingly has a higher LoS wMAPE compared to the NLoS wMAPE while the LoS MAE is lower than the NLoS MAE. This means that the model makes larger absolute errors in non-line-of-sight conditions, but in line-of-sight, even small absolute errors result in a higher relative error. This suggests that many of the errors in LoS occur at locations with small true values — a visual analysis showed that the errors happen at the front of buildings which is very atypically and questionable why this happens. Still, the 7th variation achieves SOTA in both NLoS errors. \\
		The 3rd variation achieves SOTA in LoS MAE but has a very high LoS wMAPE. The visual samples show that the error is made in the area by the source. The source is not clearly predicted.
		%which means that the errors in Line-of-Sight occur in regions with small ground truth values, where even small absolute errors lead to large relative errors. 
		
		\begin{table}[h!]
			\centering
			\begin{tabular}{|l|c|c|c|c|}
				\hline
				\textbf{Model} & \textbf{LoS MAE} & \textbf{NLoS MAE} & \textbf{LoS wMAPE} & \textbf{NLoS wMAPE} \\
				\hline
				DepthAnything & 11.33 & 5.08 & 61.27 & 118.79 \\
				HexaWaveNet 1 & 37.01 & 11.84 & 180.70 & 31.07 \\
				HexaWaveNet 2 & 37.58 & 16.41 & 183.58 & 42.98 \\
				HexaWaveNet 3 & \textbf{1.84} & 6.03 & 44.01 & 15.70 \\
				HexaWaveNet 7 & 2.60 & \textbf{3.25} & 18.06 & \textbf{8.53} \\
				HexaWaveNet 8 & 2.90 & 16.69 & 41.16 & 43.58 \\
				Pix2Pix & 2.14 & 4.79 & 11.30 & 30.67 \\
				DDBM & 1.93 & 6.38 & 18.34 & 79.13 \\
				Full Glow & 2.06 & 3.64 & \textbf{8.98} & 22.69 \\
				\hline
			\end{tabular}
			\caption{Performance comparison of the new explored models on reflection Physgen dataset.}
			\label{tab:experiment_architecture_results}
		\end{table}
		\FloatBarrier
		
	\clearpage
		
	\section{Residual Design/Architecture}
	\label{sec:experiments-residual_architecture}
		% Description
		While changing the input distribution did work, yet it did not show the wanted improvements, this experiment tries a residual approach and attempts to make the output distribution less complicated by extracting the reflection and splitting the task into two subtasks. One is to predict the base propagation and the other is the prediction of only the reflection or more explicitly the difference between the reflection propagation (which includes the base propagation) and the base propagation. \\This design is shown in figure \ref{fig:experiment_residual_design}. 
		The prediction model is not further specified and could be any model. In this experiment, DepthAnything and pix2pix model were used.
		\begin{figure}[H]
			\centering
			\includegraphics[width=1.0\textwidth]{img/experiment_residual_design.png}
			\caption[On the left is the residual design, where the task is split into 2 sub-tasks. The merging is optionally a CNN head or just a formula. On the right is the input and output of the only reflection part shown.]{On the left is the residual design, where the task is split into 2 sub-tasks. The merging is optionally a CNN head or just a formula. On the right is the input and output of the only reflection part shown.}
			\label{fig:experiment_residual_design}
		\end{figure}
		\FloatBarrier
		% Implementation & Challenges
		The creation of the reflection-only image was pretty straight. Subtract the base propagation from the reflection propagation. The value range then was about -1.0 and 0.0, so the images were transformed to a range of 0.0 to 1.0. With that creation process the reverse step with a reflection-only prediction is simple as the following formula shows:
		\begin{align}  % equation % \Rightarrow 
			\label{reflection-only-formula}
			y = (x - z) \cdot -1 \\
			\frac{y}{-1} = x - z \\
			\left(-y \right) + z = x
		\end{align}
		\noindent where:
		\begin{itemize}[itemsep=1mm, parsep=0pt]
			\item $y$ is a reflection-only propagation image
			\item $x$ is a reflection propagation image
			\item $z$ is a base propagation image
		\end{itemize}
		
		So with this reverse calculation, we could combine the base propagation with the reflection-only propagation and receive the reflection propagation.\\
		Alternatively, a simple CNN head was trained for the combination. The CNN takes an image with 2 channels (one for the base and one for the reflection-only propagation), extends the feature maps to 64 with a 3x3 kernel, and then in a second 1x1 kernel CNN layer projects the 64 channels to 1 channel. Only a 1x1 projection without the extraction of features could be also a good or better alternative.\\
		\\
		A big challenge is the reflection-only images which have a difficult distribution, as shown in figure \ref{fig:experiment_residual_hist}. The first results already show that models tend strongly to just learn the biggest part of the distribution (a complete gray/black image) and quickly reach a local minimum. Through the small loss that comes from the reflection, it is hard for the models to pass the local minima. The loss was extended with a loss for the variance and a loss for the min-max value range of the images. This already helped pass the local minima. Another technique was a weighting of the loss with the inverted histogram (only works with losses like l1 where the loss is calculated pixel-wise).
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=1.0\textwidth]{img/experiment_residual_hist.png}
			\caption[Pixel-Value distribution of reflection only image.]{Pixel-Value distribution of reflection only image.}
			\label{fig:experiment_residual_hist}
		\end{figure}
		\FloatBarrier
		
		It was also not easy to implement the residual model design inside the Pix2pix pipeline. The pipeline takes much control over how to handle the model and the dataset and the pipeline is not designed for a multi-model with multiple datasets.\\
		A possible approach was to train 3 single models and combine them later, but the loading of such a multi-model later also was not supported by the pipeline.\\
		So a multi-model with its data handling was implemented which trains 3 models in one training with the Physgen dataset. The model is therefore not optimal for the pipeline and can not use all benefits from the pipeline but still work. Currently, this comes with a limitation of a batch size of 1 but this could be fixed in the future.
		
		
		% Results
		The first obvious result is that the CNN head did not work for both of the models. While the formula \ref{reflection-only-formula} did worked.\\
		The DepthAnything successfully learned an approximation of the target distribution but still was away from SOTA. Visually the output from the pix2pix models was far away from the target and this also is represented by the LoS MAW and LoS wMAPE. However, the Pix2pix model with WGAN-GP achieved SOTA results in the NLoS errors. The output images exhibit high-frequency noise with drastic per-pixel changes which seems a successful technique to achieve good errors in these low-value NLoS areas. The models would need more engineering to get more precise and result with fewer fluctuations. The combination should be adjusted as well as the loss and the weights of the loss.\\
		Table \ref{tab:experiment_residual_design_results} shows the results with the other base model results.
		\begin{table}[h!]
			\centering
			\begin{tabular}{|l|c|c|c|c|}
				\hline
				\textbf{Model} & \textbf{LoS MAE} & \textbf{NLoS MAE} & \textbf{LoS wMAPE} & \textbf{NLoS wMAPE} \\
				\hline
				DepthAnything & 11.33 & 5.08 & 61.27 & 118.79 \\
				Residual DepthAnything & 7.70 & 11.96 & 54.39 & 142.19 \\
				Residual Pix2Pix GAN & 37.41 & 12.38 & 183.20 & 32.48 \\
				Residual Pix2Pix WGAN-GP & 33.48 & \underline{3.83} & 166.41 & \underline{10.03} \\
				HexaWaveNet 7 & 2.60 & \textbf{3.25} & 18.06 & \textbf{8.53} \\
				Pix2Pix & 2.14 & 4.79 & 11.30 & 30.67 \\
				DDBM & \textbf{1.93} & 6.38 & 18.34 & 79.13 \\
				Full Glow & 2.06 & 3.64 & \textbf{8.98} & 22.69 \\
				\hline
			\end{tabular}
			\caption{Performance comparison of the models with residual design on reflection Physgen dataset.}
			\label{tab:experiment_residual_design_results}
		\end{table}
		\FloatBarrier
		
	\clearpage
	
	\section{Only Reflection with few Buildings}
	\label{sec:experiments-only_reflections_with_few_buildings}
		% Description
		As the results of the experiment with a residual design in section \ref{sec:experiments-residual_architecture} shown, the models still struggle to learn a precise representation of the reflections although good error values are achieved. This experiment connects to these results and tries to focus only on the prediction of the reflection-only propagation which is described in section \ref{sec:experiments-residual_architecture} and shown in figure \ref{fig:experiment_residual_design}. Also, the dataset is changed to the dataset with fewer random buildings described in section \ref{sec:experiments-input-abstraction} to make the distributions simpler.\\
		The used weighting technique is extended to a mask built from the target to focus the loss only towards the reflections and not the background, so that the distribution to learn gets even simpler and more balanced.\\
		Also, much engineering was done to find fitting loss weights, a new loss against blur, and improved train hyper-parameter. \\
		This leads to 5 different models:
		\begin{itemize}[itemsep=1mm, parsep=0pt]
			\item Pix2pix with standard L1 Loss and Wasserstein Gradient Penalty Loss
			\item Pix2pix with masked L1 Loss and Wasserstein Gradient Penalty Loss
			\item Pix2pix with a masked combined loss (L1, Shape-Aware, Variance, Min-Max-Range, Blur)
			\item Hexa-Wave-Net 7 with a combined loss (L1, Shape-Aware, Variance, Min-Max-Range, Blur)
			\item Hexa-Wave-Net 7 with a masked combined loss (L1, Shape-Aware, Variance, Min-Max-Range, Blur)
		\end{itemize}
		
		% Implementation & Challenges
		
		
		% Results
		FIXME -> Add Results and the meaning from them
		
	
	%\section{Mask Sampling}
	%\label{sec:experiments-masking}
	%	FIXME -> what experiments did you...will you done?
	
	

