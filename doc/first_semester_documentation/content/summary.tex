\chapter{Summary}
\label{cha:summary}

	% TO DO: 
	%     - Fill the gaps
	
	
	\section{Achievements}
	\label{sec:sum-reached}
	Most of the here proposed experiments yield improvement yet with much space for more improvement.\\
	The experiment about input abstraction showed that an intermediate input can help focus on the complex area. The Wasserstein loss with gradient penalty stabilizes the training and speeds up convergence.\\
	While DepthAnything achieved only medium results in different scenarios, the HexaWaveNet variation 7 achieved promising results which still have manifestly potential for optimization.\\
	The residual design models showed strong performance but with unwanted predictions. With more engineering, this architecture could have much potential.\\
	This idea is carried out in the experiment \ref{sec:experiments-only_reflections_with_few_buildings}, where *FIXME* is shown.\\
	Overall this project did not reach the end of improvements. However, this project showed that there are multiple ways to improve the performance.
	
	\section{Future}
	\label{sec:sum-future}
	In the future, the experiments can be continued because there is still much to test that could improve performance, including new residual techniques, new architectures such as an improved U-Net or stacked U-Nets, and new losses, for example, physics-based ones.\\
	Additionally, existing approaches can be enhanced by optimizing their parameters and settings, as well as by combining different approaches. \\
	It is also possible to lower the distribution complexity even further but the question is how exactly. The reflection could be abstracted into an angle number or something like that. Or maybe the end-to-end learning with masking the complex part could also be promising yet questionable if the models struggle even with learning solely the reflection.
	
	
	