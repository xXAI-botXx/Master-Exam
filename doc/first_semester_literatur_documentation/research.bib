@misc{ronneberger_u-net_2015,
	title = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1505.04597},
	doi = {10.48550/ARXIV.1505.04597},
	shorttitle = {U-Net},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the {ISBI} challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and {DIC}) we won the {ISBI} cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent {GPU}. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	publisher = {{arXiv}},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	urldate = {2025-05-12},
	date = {2015},
	note = {Version Number: 1},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences},
}

@misc{ladwig_ai-guided_2024,
	title = {{AI}-{Guided} {Noise} {Reduction} for {Urban} {Geothermal} {Drilling}},
	url = {https://journals.hs-offenburg.de/index.php/urai/article/view/7},
	abstract = {Urban geothermal energy production plays a critical role in achieving global climate objectives. However, drilling operations in densely populated areas generate significant noise pollution, posing challenges to community acceptance and regulatory compliance. This research presents an artificial intelligence-driven approach to dynamically reduce noise emissions during geothermal drilling. We integrate Deep Reinforcement Learning (DRL) with generative neural network models to provide real-time recommendations for optimal drilling parameters. Specifically, the Drill-LSTM model forecasts future machine states, while the Sound-GAN framework predicts sound propagation based on varying operational conditions. These models feed into a DRL-Agent that learns to balance drilling efficiency with noise minimization. Additionally, an interactive assistance system GUI presents predictions, forecasts, and recommendations to human operators, facilitating informed decision-making. Our system demonstrates significant potential in reducing noise levels, enhancing operational efficiency, and fostering greater acceptance of urban geothermal projects. Future work will focus on refining the models and validating the system in real-world drilling scenarios.},
	language = {English},
	urldate = {2025-04-22},
	author = {Ladwig, Daniel and Spitznagel, Martin and Vaillant, Jan and Dorer, Klaus and Keuper, Janis},
	month = oct,
	year = {2024},
	file = {PDF:C\:\\Users\\tobia\\Zotero\\storage\\FTA6Q8ED\\Ladwig et al. - 2024 - AI-Guided Noise Reduction for Urban Geothermal Drilling.pdf:application/pdf},
}

@misc{spitznagel_urban_2024,
	title = {{URBAN} {SOUND} {PROPAGATION} : {A} {BENCHMARK} {FOR} 1-{STEP} {GENERATIVE} {MODELING} {OF} {COMPLEX} {PHYSICAL} {SYSTEMS}},
	url = {https://arxiv.org/abs/2403.10904},
	abstract = {Data-driven modeling of complex physical systems is receiving a growing amount of attention in
the simulation and machine learning communities. Since most physical simulations are based on
compute-intensive, iterative implementations of differential equation systems, a (partial) replacement
with learned, 1-step inference models has the potential for significant speedups in a wide range
of application areas. In this context, we present a novel benchmark for the evaluation of 1-step
generative learning models in terms of speed and physical correctness.
Our Urban Sound Propagation benchmark is based on the physically complex and practically relevant,
yet intuitively easy to grasp task of modeling the 2d propagation of waves from a sound source in
an urban environment. We provide a dataset with 100k samples, where each sample consists of
pairs of real 2d building maps drawn from OpenStreetmap , a parameterized sound source, and a
simulated ground truth sound propagation for the given scene. The dataset provides four different
simulation tasks with increasing complexity regarding reflection, diffraction and source variance.
A first baseline evaluation of common generative U-Net, GAN and Diffusion models shows, that
while these models are very well capable of modeling sound propagations in simple cases, the
approximation of sub-systems represented by higher order equations systematically fails.
Information about the dataset, download instructions and source codes are provided on our website:
https://www.urban-sound-data.org .},
	language = {English},
	urldate = {2025-04-22},
	author = {Spitznagel, Martin and Keuper, Janis},
	month = mar,
	year = {2024},
	file = {PDF:C\:\\Users\\tobia\\Zotero\\storage\\NHMLC5XN\\Spitznagel und Keuper - 2024 - URBAN SOUND PROPAGATION  A BENCHMARK FOR 1-STEP GENERATIVE MODELING OF COMPLEX PHYSICAL SYSTEMS.pdf:application/pdf},
}

@misc{martin_spitznagel_physicsgen_2025,
	title = {{PhysicsGen}: {Can} {Generative} {Models} {Learn} from {Images} to {Predict} {Complex} {Physical} {Relations}?},
	url = {https://arxiv.org/abs/2503.05333},
	abstract = {The image-to-image translation abilities of generative learning models have recently made significant progress in the estimation of complex (steered) mappings between image distributions. While appearance based tasks like image in-painting or style transfer have been studied at length, we propose to investigate the potential of generative models in the context of physical simulations. Providing a dataset of 300k image-pairs and baseline evaluations for three different physical simulation tasks, we propose a benchmark to investigate the following research questions: i) are generative models able to learn complex physical relations from input-output image pairs? ii) what speedups can be achieved by replacing differential equation based simulations? While baseline evaluations of different current models show the potential for high speedups (ii), these results also show strong limitations toward the physical correctness (i). This underlines the need for new methods to enforce physical correctness. Data, baseline models and evaluation code this http URL.},
	language = {English},
	urldate = {2025-04-22},
	author = {{Martin Spitznagel} and {Jan Vaillant} and {Janis Keuper}},
	month = mar,
	year = {2025},
	file = {PDF:C\:\\Users\\tobia\\Zotero\\storage\\53LT6LQL\\Martin Spitznagel et al. - 2025 - PhysicsGen Can Generative Models Learn from Images to Predict Complex Physical Relations.pdf:application/pdf},
}

@misc{achim_eckerle_evaluierung_2025,
	title = {Evaluierung der physikalischen {Korrektheit} von {Normalizing} {Flows} bei {Bild}-zu-{Bild}-{Transformationen} in {Schallausbreitungssimulationen}},
	abstract = {This study evaluates the physical accuracy of Normalizing Flows for image-to-image transformations in urban sound propagation simulations. Various simulated datasets are employed, encompassing sound reflections and sound diffractions, as well as additional factors such as temperature, humidity, and decibel levels. These datasets are trained on the invertible Normalizing Flow architecture Full Glow. The results are assessed using the Mean Absolute Error (MAE), the Weighted Mean Absolute Percentage Error (wMAPE), and the Structural Similarity Index (SSIM), capturing both local deviations and
global structural properties of the generated sound maps. The Full Glow model achieves, for instance, a Non-Line-of-Sight (NLoS) MAE of 0.65 and a Line-of-Sight (LoS) MAE of 1.84in baseline scenarios, indicating significantly more accurate predictions than previous architectures. In diffraction cases, the LoS MAE of 0.79 and the NLoS MAE of 2.63 suggest that diffraction phenomena are reliably captured.
Although reflection tasks also show improved accuracy compared to reference methods (NLoS MAE 3.64 ), there remains scope for optimization. Overall, these results confirm that Normalizing Flows can generate physically plausible sound distributions while reducing computational demands relative to
classical simulations. However, simultaneously modeling multiple effects—particularly when reflection, diffraction, and additional parameters intersect—remains challenging. This indicates that expanding the training data and refining physical constraints could further enhance performance.},
	language = {Deutsch},
	author = {{Achim Eckerle}},
	month = jan,
	year = {2025},
	file = {PDF:C\:\\Users\\tobia\\Zotero\\storage\\473E22LN\\Achim Eckerle - 2025 - Evaluierung der physikalischen Korrektheit von Normalizing Flows bei Bild-zu-Bild-Transformationen i.pdf:application/pdf},
}

@misc{ning_decomposing_2023,
	title = {Decomposing and {Coupling} {Saliency} {Map} for {Lesion} {Segmentation} in {Ultrasound} {Images}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2308.00947},
	doi = {10.48550/ARXIV.2308.00947},
	abstract = {Complex scenario of ultrasound image, in which adjacent tissues (i.e., background) share similar intensity with and even contain richer texture patterns than lesion region (i.e., foreground), brings a unique challenge for accurate lesion segmentation. This work presents a decomposition-coupling network, called DC-Net, to deal with this challenge in a (foreground-background) saliency map disentanglement-fusion manner. The DC-Net consists of decomposition and coupling subnets, and the former preliminarily disentangles original image into foreground and background saliency maps, followed by the latter for accurate segmentation under the assistance of saliency prior fusion. The coupling subnet involves three aspects of fusion strategies, including: 1) regional feature aggregation (via differentiable context pooling operator in the encoder) to adaptively preserve local contextual details with the larger receptive field during dimension reduction; 2) relation-aware representation fusion (via cross-correlation fusion module in the decoder) to efficiently fuse low-level visual characteristics and high-level semantic features during resolution restoration; 3) dependency-aware prior incorporation (via coupler) to reinforce foreground-salient representation with the complementary information derived from background representation. Furthermore, a harmonic loss function is introduced to encourage the network to focus more attention on low-confidence and hard samples. The proposed method is evaluated on two ultrasound lesion segmentation tasks, which demonstrates the remarkable performance improvement over existing state-of-the-art methods.},
	urldate = {2025-04-28},
	publisher = {arXiv},
	author = {Ning, Zhenyuan and Mao, Yixiao and Feng, Qianjin and Zhong, Shengzhou and Zhang, Yu},
	year = {2023},
	note = {Version Number: 1},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, Image and Video Processing (eess.IV), Machine Learning (cs.LG)},
	file = {PDF:C\:\\Users\\tobia\\Zotero\\storage\\7374B4L2\\Ning et al. - 2023 - Decomposing and Coupling Saliency Map for Lesion Segmentation in Ultrasound Images.pdf:application/pdf},
}

@inproceedings{gill_focus_2021,
	address = {Montreal, QC, Canada},
	title = {Focus {Guided} {Light} {Field} {Saliency} {Estimation}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-6654-3589-5},
	url = {https://ieeexplore.ieee.org/document/9465428/},
	doi = {10.1109/QoMEX51781.2021.9465428},
	urldate = {2025-04-28},
	booktitle = {2021 13th {International} {Conference} on {Quality} of {Multimedia} {Experience} ({QoMEX})},
	publisher = {IEEE},
	author = {Gill, Ailbhe and Zerman, Emin and Alain, Martin and Le Pendu, Mikael and Smolic, Aljosa},
	month = jun,
	year = {2021},
	pages = {213--218},
	file = {PDF:C\:\\Users\\tobia\\Zotero\\storage\\AVBMMK22\\Gill et al. - 2021 - Focus Guided Light Field Saliency Estimation.pdf:application/pdf},
}

@article{ismail_improving_2021,
	title = {Improving {Deep} {Learning} {Interpretability} by {Saliency} {Guided} {Training}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2111.14338},
	doi = {10.48550/ARXIV.2111.14338},
	abstract = {Saliency methods have been widely used to highlight important input features in model predictions. Most existing methods use backpropagation on a modified gradient function to generate saliency maps. Thus, noisy gradients can result in unfaithful feature attributions. In this paper, we tackle this issue and introduce a \{{\textbackslash}it saliency guided training\}procedure for neural networks to reduce noisy gradients used in predictions while retaining the predictive performance of the model. Our saliency guided training procedure iteratively masks features with small and potentially noisy gradients while maximizing the similarity of model outputs for both masked and unmasked inputs. We apply the saliency guided training procedure to various synthetic and real data sets from computer vision, natural language processing, and time series across diverse neural architectures, including Recurrent Neural Networks, Convolutional Networks, and Transformers. Through qualitative and quantitative evaluations, we show that saliency guided training procedure significantly improves model interpretability across various domains while preserving its predictive performance.},
	urldate = {2025-04-28},
	author = {Ismail, Aya Abdelsalam and Bravo, Héctor Corrada and Feizi, Soheil},
	year = {2021},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, Machine Learning (cs.LG), Artificial Intelligence (cs.AI)},
	file = {PDF:C\:\\Users\\tobia\\Zotero\\storage\\KLGYYGZW\\Ismail et al. - 2021 - Improving Deep Learning Interpretability by Saliency Guided Training.pdf:application/pdf},
}

@misc{lin_focal_2017,
	title = {Focal {Loss} for {Dense} {Object} {Detection}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1708.02002},
	doi = {10.48550/ARXIV.1708.02002},
	abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
	urldate = {2025-04-28},
	publisher = {arXiv},
	author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollár, Piotr},
	year = {2017},
	note = {Version Number: 2},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
	file = {PDF:C\:\\Users\\tobia\\Zotero\\storage\\BRAXA3QW\\Tsung-Yi Lin et al. - 2018 - Focal Loss for Dense Object Detection.pdf:application/pdf},
}

@inproceedings{huang_masked_2022,
	title = {Masked {Generative} {Adversarial} {Networks} are {Data}-{Efficient} {Generation} {Learners}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/0efcb1885b8534109f95ca82a5319d25-Paper-Conference.pdf},
	abstract = {This paper shows that masked generative adversarial network (MaskedGAN) is robust image generation learners with limited training data. The idea of MaskedGAN is simple: it randomly masks out certain image information for effective GAN training with limited data. We develop two masking strategies that work along orthogonal dimensions of training images, including a shifted spatial masking that masks the images in spatial dimensions with random shifts, and a balanced spectral masking that masks certain image spectral bands with self-adaptive probabilities. The two masking strategies complement each other which together encourage more challenging holistic learning from limited training data, ultimately suppressing trivial solutions and failures in GAN training. Albeit simple, extensive experiments show that MaskedGAN achieves superior performance consistently across different network architectures (e.g., CNNs including BigGAN and StyleGAN-v2 and Transformers including TransGAN and GANformer) and datasets (e.g., CIFAR-10, CIFAR-100, ImageNet, 100-shot, AFHQ, FFHQ and Cityscapes).},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Huang, Jiaxing and Cui, Kaiwen and Guan, Dayan and Xiao, Aoran and Zhan, Fangneng and Lu, Shijian and Liao, Shengcai and Xing, Eric},
	editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
	year = {2022},
	pages = {2154--2167},
	file = {PDF:C\:\\Users\\tobia\\Zotero\\storage\\GGPRIXPW\\Jiaxing Huang et al. - 2022 - Masked Generative Adversarial Networks are Data-Efficient Generation Learners.pdf:application/pdf},
}

@article{liu_sgfusion_2023,
	title = {{SGFusion}: {A} saliency guided deep-learning framework for pixel-level image fusion},
	volume = {91},
	issn = {15662535},
	shorttitle = {{SGFusion}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1566253522001683},
	doi = {10.1016/j.inffus.2022.09.030},
	language = {en},
	urldate = {2025-04-28},
	journal = {Information Fusion},
	author = {Liu, Jinyang and Dian, Renwei and Li, Shutao and Liu, Haibo},
	month = mar,
	year = {2023},
	pages = {205--214},
	file = {PDF:C\:\\Users\\tobia\\Zotero\\storage\\ZSDCVP3X\\Liu et al. - 2023 - SGFusion A saliency guided deep-learning framework for pixel-level image fusion.pdf:application/pdf},
}

@misc{shrivastava_training_2016,
	title = {Training {Region}-based {Object} {Detectors} with {Online} {Hard} {Example} {Mining}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1604.03540},
	doi = {10.48550/ARXIV.1604.03540},
	abstract = {The field of object detection has made significant advances riding on the wave of region-based ConvNets, but their training procedure still includes many heuristics and hyperparameters that are costly to tune. We present a simple yet surprisingly effective online hard example mining (OHEM) algorithm for training region-based ConvNet detectors. Our motivation is the same as it has always been -- detection datasets contain an overwhelming number of easy examples and a small number of hard examples. Automatic selection of these hard examples can make training more effective and efficient. OHEM is a simple and intuitive algorithm that eliminates several heuristics and hyperparameters in common use. But more importantly, it yields consistent and significant boosts in detection performance on benchmarks like PASCAL VOC 2007 and 2012. Its effectiveness increases as datasets become larger and more difficult, as demonstrated by the results on the MS COCO dataset. Moreover, combined with complementary advances in the field, OHEM leads to state-of-the-art results of 78.9\% and 76.3\% mAP on PASCAL VOC 2007 and 2012 respectively.},
	urldate = {2025-04-28},
	publisher = {arXiv},
	author = {Shrivastava, Abhinav and Gupta, Abhinav and Girshick, Ross},
	year = {2016},
	note = {Version Number: 1},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, Machine Learning (cs.LG)},
	file = {PDF:C\:\\Users\\tobia\\Zotero\\storage\\MNIQ458E\\Shrivastava et al. - 2016 - Training Region-based Object Detectors with Online Hard Example Mining.pdf:application/pdf},
}

@inproceedings{ren_learning_2018,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Learning to {Reweight} {Examples} for {Robust} {Deep} {Learning}},
	volume = {80},
	url = {https://proceedings.mlr.press/v80/ren18a.html},
	abstract = {Deep neural networks have been shown to be very powerful modeling tools for many supervised learning tasks involving complex input patterns. However, they can also easily overfit to training set biases and label noises. In addition to various regularizers, example reweighting algorithms are popular solutions to these problems, but they require careful tuning of additional hyperparameters, such as example mining schedules and regularization hyperparameters. In contrast to past reweighting methods, which typically consist of functions of the cost value of each example, in this work we propose a novel meta-learning algorithm that learns to assign weights to training examples based on their gradient directions. To determine the example weights, our method performs a meta gradient descent step on the current mini-batch example weights (which are initialized from zero) to minimize the loss on a clean unbiased validation set. Our proposed method can be easily implemented on any type of deep network, does not require any additional hyperparameter tuning, and achieves impressive performance on class imbalance and corrupted label problems where only a small amount of clean validation data is available.},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Ren, Mengye and Zeng, Wenyuan and Yang, Bin and Urtasun, Raquel},
	editor = {Dy, Jennifer and Krause, Andreas},
	month = jul,
	year = {2018},
	pages = {4334--4343},
	file = {PDF:C\:\\Users\\tobia\\Zotero\\storage\\BWNM7PMK\\Ren et al. - 2018 - Learning to Reweight Examples for Robust Deep Learning.pdf:application/pdf},
}

@inproceedings{bengio_curriculum_2009,
	address = {Montreal Quebec Canada},
	title = {Curriculum learning},
	isbn = {978-1-60558-516-1},
	url = {https://dl.acm.org/doi/10.1145/1553374.1553380},
	doi = {10.1145/1553374.1553380},
	abstract = {Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them "curriculum learning". In the context of recent research studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neural networks), we explore curriculum learning in various set-ups. The experiments show that significant improvements in generalization can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).},
	language = {en},
	urldate = {2025-04-28},
	booktitle = {Proceedings of the 26th {Annual} {International} {Conference} on {Machine} {Learning}},
	publisher = {ACM},
	author = {Bengio, Yoshua and Louradour, Jérôme and Collobert, Ronan and Weston, Jason},
	month = jun,
	year = {2009},
	pages = {41--48},
	file = {PDF:C\:\\Users\\tobia\\Zotero\\storage\\RPK92EZP\\Bengio et al. - 2009 - Curriculum learning.pdf:application/pdf},
}

@book{allgower_introduction_2003,
	title = {Introduction to {Numerical} {Continuation} {Methods}},
	isbn = {978-0-89871-544-6 978-0-89871-915-4},
	url = {http://epubs.siam.org/doi/book/10.1137/1.9780898719154},
	abstract = {Training machine learning models in a meaningful order, from the easy samples to the hard ones, using curriculum learning can provide performance improvements over the standard training approach based on random data shuffling, without any additional computational costs. Curriculum learning strategies have been successfully employed in all areas of machine learning, in a wide range of tasks. However, the necessity of finding a way to rank the samples from easy to hard, as well as the right pacing function for introducing more difficult data can limit the usage of the curriculum approaches. In this survey, we show how these limits have been tackled in the literature, and we present different curriculum learning instantiations for various tasks in machine learning. We construct a multi-perspective taxonomy of curriculum learning approaches by hand, considering various classification criteria. We further build a hierarchical tree of curriculum learning methods using an agglomerative clustering algorithm, linking the discovered clusters with our taxonomy. At the end, we provide some interesting directions for future work.},
	language = {en},
	urldate = {2025-04-28},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Allgower, Eugene L. and Georg, Kurt},
	month = jan,
	year = {2003},
	doi = {10.1137/1.9780898719154},
	file = {PDF:C\:\\Users\\tobia\\Zotero\\storage\\RYBAEYHQ\\Allgower und Georg - 2003 - Introduction to Numerical Continuation Methods.pdf:application/pdf},
}

@article{wang_survey_2021,
	title = {A {Survey} on {Curriculum} {Learning}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {0162-8828, 2160-9292, 1939-3539},
	url = {https://ieeexplore.ieee.org/document/9392296/},
	doi = {10.1109/TPAMI.2021.3069908},
	urldate = {2025-04-28},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Wang, Xin and Chen, Yudong and Zhu, Wenwu},
	year = {2021},
	pages = {1--1},
	file = {Eingereichte Version:C\:\\Users\\tobia\\Zotero\\storage\\ITDZ8PUD\\Wang et al. - 2021 - A Survey on Curriculum Learning.pdf:application/pdf},
}

@misc{wang_experts_2023,
	title = {An {Expert}'s {Guide} to {Training} {Physics}-informed {Neural} {Networks}},
	copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
	url = {https://arxiv.org/abs/2308.08468},
	doi = {10.48550/ARXIV.2308.08468},
	abstract = {Physics-informed neural networks (PINNs) have been popularized as a deep learning framework that can seamlessly synthesize observational data and partial differential equation (PDE) constraints. Their practical effectiveness however can be hampered by training pathologies, but also oftentimes by poor choices made by users who lack deep learning expertise. In this paper we present a series of best practices that can significantly improve the training efficiency and overall accuracy of PINNs. We also put forth a series of challenging benchmark problems that highlight some of the most prominent difficulties in training PINNs, and present comprehensive and fully reproducible ablation studies that demonstrate how different architecture choices and training strategies affect the test accuracy of the resulting models. We show that the methods and guiding principles put forth in this study lead to state-of-the-art results and provide strong baselines that future studies should use for comparison purposes. To this end, we also release a highly optimized library in JAX that can be used to reproduce all results reported in this paper, enable future research studies, as well as facilitate easy adaptation to new use-case scenarios.},
	urldate = {2025-04-28},
	publisher = {arXiv},
	author = {Wang, Sifan and Sankaran, Shyam and Wang, Hanwen and Perdikaris, Paris},
	year = {2023},
	note = {Version Number: 1},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Computational Physics (physics.comp-ph), FOS: Mathematics, FOS: Physical sciences, Numerical Analysis (math.NA)},
	file = {PDF:C\:\\Users\\tobia\\Zotero\\storage\\SIE9CRVQ\\Wang et al. - 2023 - An Expert's Guide to Training Physics-informed Neural Networks.pdf:application/pdf},
}

@misc{robinson_physics_2022,
	title = {Physics guided neural networks for modelling of non-linear dynamics},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2205.06858},
	doi = {10.48550/ARXIV.2205.06858},
	abstract = {The success of the current wave of artificial intelligence can be partly attributed to deep neural networks, which have proven to be very effective in learning complex patterns from large datasets with minimal human intervention. However, it is difficult to train these models on complex dynamical systems from data alone due to their low data efficiency and sensitivity to hyperparameters and initialisation. This work demonstrates that injection of partially known information at an intermediate layer in a DNN can improve model accuracy, reduce model uncertainty, and yield improved convergence during the training. The value of these physics-guided neural networks has been demonstrated by learning the dynamics of a wide variety of nonlinear dynamical systems represented by five well-known equations in nonlinear systems theory: the Lotka-Volterra, Duffing, Van der Pol, Lorenz, and Henon-Heiles systems.},
	urldate = {2025-04-28},
	publisher = {arXiv},
	author = {Robinson, Haakon and Pawar, Suraj and Rasheed, Adil and San, Omer},
	year = {2022},
	note = {Version Number: 1},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), FOS: Physical sciences, Chaotic Dynamics (nlin.CD)},
	file = {PDF:C\:\\Users\\tobia\\Zotero\\storage\\DIIY6HJU\\Robinson et al. - 2022 - Physics guided neural networks for modelling of non-linear dynamics.pdf:application/pdf},
}

@misc{daw_physics-guided_2017,
	title = {Physics-guided {Neural} {Networks} ({PGNN}): {An} {Application} in {Lake} {Temperature} {Modeling}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Physics-guided {Neural} {Networks} ({PGNN})},
	url = {https://arxiv.org/abs/1710.11431},
	doi = {10.48550/ARXIV.1710.11431},
	abstract = {This paper introduces a framework for combining scientific knowledge of physics-based models with neural networks to advance scientific discovery. This framework, termed physics-guided neural networks (PGNN), leverages the output of physics-based model simulations along with observational features in a hybrid modeling setup to generate predictions using a neural network architecture. Further, this framework uses physics-based loss functions in the learning objective of neural networks to ensure that the model predictions not only show lower errors on the training set but are also scientifically consistent with the known physics on the unlabeled set. We illustrate the effectiveness of PGNN for the problem of lake temperature modeling, where physical relationships between the temperature, density, and depth of water are used to design a physics-based loss function. By using scientific knowledge to guide the construction and learning of neural networks, we are able to show that the proposed framework ensures better generalizability as well as scientific consistency of results. All the code and datasets used in this study have been made available on this link {\textbackslash}url\{https://github.com/arkadaw9/PGNN\}.},
	urldate = {2025-04-28},
	publisher = {arXiv},
	author = {Daw, Arka and Karpatne, Anuj and Watkins, William and Read, Jordan and Kumar, Vipin},
	year = {2017},
	note = {Version Number: 3},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Physical sciences, Data Analysis, Statistics and Probability (physics.data-an), Machine Learning (stat.ML)},
	file = {PDF:C\:\\Users\\tobia\\Zotero\\storage\\6FMFBT87\\Daw et al. - 2017 - Physics-guided Neural Networks (PGNN) An Application in Lake Temperature Modeling.pdf:application/pdf},
}

@misc{esser_taming_2020,
	title = {Taming {Transformers} for {High}-{Resolution} {Image} {Synthesis}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2012.09841},
	doi = {10.48550/ARXIV.2012.09841},
	abstract = {Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use CNNs to learn a context-rich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the first results on semantically-guided synthesis of megapixel images with transformers and obtain the state of the art among autoregressive models on class-conditional ImageNet. Code and pretrained models can be found at https://github.com/CompVis/taming-transformers .},
	urldate = {2025-04-29},
	publisher = {arXiv},
	author = {Esser, Patrick and Rombach, Robin and Ommer, Björn},
	year = {2020},
	note = {Version Number: 3},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
	file = {PDF:C\:\\Users\\tobia\\Zotero\\storage\\ZVUY5YMV\\Esser et al. - 2020 - Taming Transformers for High-Resolution Image Synthesis.pdf:application/pdf},
}

@misc{sitzmann_implicit_2020,
	title = {Implicit {Neural} {Representations} with {Periodic} {Activation} {Functions}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2006.09661},
	doi = {10.48550/ARXIV.2006.09661},
	abstract = {Implicitly defined, continuous, differentiable signal representations parameterized by neural networks have emerged as a powerful paradigm, offering many possible benefits over conventional representations. However, current network architectures for such implicit neural representations are incapable of modeling signals with fine detail, and fail to represent a signal's spatial and temporal derivatives, despite the fact that these are essential to many physical signals defined implicitly as the solution to partial differential equations. We propose to leverage periodic activation functions for implicit neural representations and demonstrate that these networks, dubbed sinusoidal representation networks or Sirens, are ideally suited for representing complex natural signals and their derivatives. We analyze Siren activation statistics to propose a principled initialization scheme and demonstrate the representation of images, wavefields, video, sound, and their derivatives. Further, we show how Sirens can be leveraged to solve challenging boundary value problems, such as particular Eikonal equations (yielding signed distance functions), the Poisson equation, and the Helmholtz and wave equations. Lastly, we combine Sirens with hypernetworks to learn priors over the space of Siren functions.},
	urldate = {2025-04-29},
	publisher = {arXiv},
	author = {Sitzmann, Vincent and Martel, Julien N. P. and Bergman, Alexander W. and Lindell, David B. and Wetzstein, Gordon},
	year = {2020},
	note = {Version Number: 1},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, Image and Video Processing (eess.IV), Machine Learning (cs.LG)},
	file = {PDF:C\:\\Users\\tobia\\Zotero\\storage\\8ICJ9KTQ\\Sitzmann et al. - 2020 - Implicit Neural Representations with Periodic Activation Functions.pdf:application/pdf},
}

@misc{li_fourier_2020,
	title = {Fourier {Neural} {Operator} for {Parametric} {Partial} {Differential} {Equations}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2010.08895},
	doi = {10.48550/ARXIV.2010.08895},
	abstract = {The classical development of neural networks has primarily focused on learning mappings between finite-dimensional Euclidean spaces. Recently, this has been generalized to neural operators that learn mappings between function spaces. For partial differential equations (PDEs), neural operators directly learn the mapping from any functional parametric dependence to the solution. Thus, they learn an entire family of PDEs, in contrast to classical methods which solve one instance of the equation. In this work, we formulate a new neural operator by parameterizing the integral kernel directly in Fourier space, allowing for an expressive and efficient architecture. We perform experiments on Burgers' equation, Darcy flow, and Navier-Stokes equation. The Fourier neural operator is the first ML-based method to successfully model turbulent flows with zero-shot super-resolution. It is up to three orders of magnitude faster compared to traditional PDE solvers. Additionally, it achieves superior accuracy compared to previous learning-based solvers under fixed resolution.},
	urldate = {2025-04-29},
	publisher = {arXiv},
	author = {Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
	year = {2020},
	note = {Version Number: 3},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), FOS: Mathematics, Numerical Analysis (math.NA)},
	file = {PDF:C\:\\Users\\tobia\\Zotero\\storage\\TVIGU3WK\\Li et al. - 2020 - Fourier Neural Operator for Parametric Partial Differential Equations.pdf:application/pdf},
}

@misc{raissi_physics_2017,
	title = {Physics {Informed} {Deep} {Learning} ({Part} {I}): {Data}-driven {Solutions} of {Nonlinear} {Partial} {Differential} {Equations}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Physics {Informed} {Deep} {Learning} ({Part} {I})},
	url = {https://arxiv.org/abs/1711.10561},
	doi = {10.48550/ARXIV.1711.10561},
	abstract = {We introduce physics informed neural networks -- neural networks that are trained to solve supervised learning tasks while respecting any given law of physics described by general nonlinear partial differential equations. In this two part treatise, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct classes of algorithms, namely continuous time and discrete time models. The resulting neural networks form a new class of data-efficient universal function approximators that naturally encode any underlying physical laws as prior information. In this first part, we demonstrate how these networks can be used to infer solutions to partial differential equations, and obtain physics-informed surrogate models that are fully differentiable with respect to all input coordinates and free parameters.},
	urldate = {2025-04-29},
	publisher = {arXiv},
	author = {Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},
	year = {2017},
	note = {Version Number: 1},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Mathematics, Numerical Analysis (math.NA), Machine Learning (stat.ML), Dynamical Systems (math.DS)},
	file = {PDF:C\:\\Users\\tobia\\Zotero\\storage\\Z5M3IB24\\Raissi et al. - 2017 - Physics Informed Deep Learning (Part I) Data-driven Solutions of Nonlinear Partial Differential Equ.pdf:application/pdf},
}

@misc{saharia_palette_2021,
	title = {Palette: {Image}-to-{Image} {Diffusion} {Models}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Palette},
	url = {https://arxiv.org/abs/2111.05826},
	doi = {10.48550/ARXIV.2111.05826},
	abstract = {This paper develops a unified framework for image-to-image translation based on conditional diffusion models and evaluates this framework on four challenging image-to-image translation tasks, namely colorization, inpainting, uncropping, and JPEG restoration. Our simple implementation of image-to-image diffusion models outperforms strong GAN and regression baselines on all tasks, without task-specific hyper-parameter tuning, architecture customization, or any auxiliary loss or sophisticated new techniques needed. We uncover the impact of an L2 vs. L1 loss in the denoising diffusion objective on sample diversity, and demonstrate the importance of self-attention in the neural architecture through empirical studies. Importantly, we advocate a unified evaluation protocol based on ImageNet, with human evaluation and sample quality scores (FID, Inception Score, Classification Accuracy of a pre-trained ResNet-50, and Perceptual Distance against original images). We expect this standardized evaluation protocol to play a role in advancing image-to-image translation research. Finally, we show that a generalist, multi-task diffusion model performs as well or better than task-specific specialist counterparts. Check out https://diffusion-palette.github.io for an overview of the results.},
	urldate = {2025-05-07},
	publisher = {arXiv},
	author = {Saharia, Chitwan and Chan, William and Chang, Huiwen and Lee, Chris A. and Ho, Jonathan and Salimans, Tim and Fleet, David J. and Norouzi, Mohammad},
	year = {2021},
	note = {Version Number: 2},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, Machine Learning (cs.LG)},
	file = {PDF:C\:\\Users\\tobia\\Zotero\\storage\\FG4YQBRN\\Saharia et al. - 2021 - Palette Image-to-Image Diffusion Models.pdf:application/pdf},
}

@article{yang_physics-informed_2020,
	title = {Physics-{Informed} {Generative} {Adversarial} {Networks} for {Stochastic} {Differential} {Equations}},
	volume = {42},
	issn = {1064-8275, 1095-7197},
	url = {https://epubs.siam.org/doi/10.1137/18M1225409},
	doi = {10.1137/18M1225409},
	language = {en},
	number = {1},
	urldate = {2025-05-07},
	journal = {SIAM Journal on Scientific Computing},
	author = {Yang, Liu and Zhang, Dongkun and Karniadakis, George Em},
	month = jan,
	year = {2020},
	pages = {A292--A317},
	file = {Eingereichte Version:C\:\\Users\\tobia\\Zotero\\storage\\JN7IRTSK\\Yang et al. - 2020 - Physics-Informed Generative Adversarial Networks for Stochastic Differential Equations.pdf:application/pdf;Physics-Informed Generative Adversarial Networks for Stochastic Differential Equations:C\:\\Users\\tobia\\Zotero\\storage\\QQPVA2RJ\\Physics-Informed Generative Adversarial Networks for Stochastic Differential Equations.pdf:application/pdf},
}

@inproceedings{yang_highly-scalable_2019,
	address = {Denver, CO, USA},
	title = {Highly-scalable, {Physics}-{Informed} {GANs} for {Learning} {Solutions} of {Stochastic} {PDEs}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-7281-6011-5},
	url = {https://ieeexplore.ieee.org/document/8945120/},
	doi = {10.1109/DLS49591.2019.00006},
	urldate = {2025-05-11},
	booktitle = {2019 {IEEE}/{ACM} {Third} {Workshop} on {Deep} {Learning} on {Supercomputers} ({DLS})},
	publisher = {IEEE},
	author = {Yang, Liu and Prabhat, Mr and Karniadakis, George and Treichler, Sean and Kurth, Thorsten and Fischer, Keno and Barajas-Solano, David and Romero, Josh and Churavy, Valentin and Tartakovsky, Alexandre and Houston, Michael},
	month = nov,
	year = {2019},
	pages = {1--11},
	file = {Eingereichte Version:C\:\\Users\\tobia\\Zotero\\storage\\PIDJ289K\\Yang et al. - 2019 - Highly-scalable, Physics-Informed GANs for Learning Solutions of Stochastic PDEs.pdf:application/pdf;Highly-scalable - Physics-Informed GANs for Learning Solutions of Stochastic PDEs:C\:\\Users\\tobia\\Zotero\\storage\\3BAM7GBS\\Highly-scalable - Physics-Informed GANs for Learning Solutions of Stochastic PDEs.pdf:application/pdf},
}

@misc{lutjens_physics-informed_2020,
	title = {Physics-informed {GANs} for {Coastal} {Flood} {Visualization}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2010.08103},
	doi = {10.48550/ARXIV.2010.08103},
	abstract = {As climate change increases the intensity of natural disasters, society needs better tools for adaptation. Floods, for example, are the most frequent natural disaster, but during hurricanes the area is largely covered by clouds and emergency managers must rely on nonintuitive flood visualizations for mission planning. To assist these emergency managers, we have created a deep learning pipeline that generates visual satellite images of current and future coastal flooding. We advanced a state-of-the-art GAN called pix2pixHD, such that it produces imagery that is physically-consistent with the output of an expert-validated storm surge model (NOAA SLOSH). By evaluating the imagery relative to physics-based flood maps, we find that our proposed framework outperforms baseline models in both physical-consistency and photorealism. While this work focused on the visualization of coastal floods, we envision the creation of a global visualization of how climate change will shape our earth.},
	urldate = {2025-05-11},
	publisher = {arXiv},
	author = {Lütjens, Björn and Leshchinskiy, Brandon and Requena-Mesa, Christian and Chishtie, Farrukh and Díaz-Rodriguez, Natalia and Boulais, Océane and Piña, Aaron and Newman, Dava and Lavin, Alexander and Gal, Yarin and Raïssi, Chedy},
	year = {2020},
	note = {Version Number: 2},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, Human-Computer Interaction (cs.HC), Image and Video Processing (eess.IV), Machine Learning (cs.LG)},
	file = {PDF:C\:\\Users\\tobia\\Zotero\\storage\\VZTS2RXI\\Lütjens et al. - 2020 - Physics-informed GANs for Coastal Flood Visualization.pdf:application/pdf},
}

@inproceedings{su_convunext_2022,
	address = {Himeji, Japan},
	title = {{ConvUNeXt}: {A} {Lightweight} {Convolutional} {Neural} {Network} for {Watercolor} {Image} {Translation}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-6654-7532-7},
	shorttitle = {{ConvUNeXt}},
	url = {https://ieeexplore.ieee.org/document/10062649/},
	doi = {10.1109/CANDARW57323.2022.00033},
	abstract = {Image-to-image transformation is the task of transforming an image from one domain to another. It includes the task of converting an image to an artistic style such as oil painting, tile art, and watercolor. These conversion techniques have rapidly advanced with the development of machine learning in recent years. This paper proposes a method for generating watercolor paintings using machine learning technologies. The proposed method uses conditional generative adversary networks (cGANs) to convert the style of input images from real to watercolor. Specifically, we propose a ConvUNeXt generator that generates high-quality watercolor images while preserving the color information of the original image. It employs an encoder-decoder structure with skip connections and introduces multiple ConvNeXt blocks to learn image features. Experimental results show that the proposed model significantly outperforms traditional rendering methods in the time consumption of image translation while achieving comparable translation performance.},
	urldate = {2025-05-11},
	booktitle = {2022 {Tenth} {International} {Symposium} on {Computing} and {Networking} {Workshops} ({CANDARW})},
	publisher = {IEEE},
	author = {Su, Hao and Huang, Jiamian and Ito, Yasuaki and Nakano, Koji},
	month = nov,
	year = {2022},
	pages = {127--133},
	file = {PDF:C\:\\Users\\tobia\\Zotero\\storage\\37Y68UQK\\Su et al. - 2022 - ConvUNeXt A Lightweight Convolutional Neural Network for Watercolor Image Translation.pdf:application/pdf},
}

@article{ma_span_2025,
	title = {{\textless}span style="font-variant:small-caps;"{\textgreater}{TransGAN}{\textless}/span{\textgreater} : {A} {Transformer}‐ {\textless}span style="font-variant:small-caps;"{\textgreater}{CNN}{\textless}/span{\textgreater} {Mixed} {Model} for {Volumetric} {\textless}span style="font-variant:small-caps;"{\textgreater}{CT}{\textless}/span{\textgreater} to {\textless}span style="font-variant:small-caps;"{\textgreater}{MRI}{\textless}/span{\textgreater} {Modality} {Translation} and {Visualization}},
	volume = {35},
	issn = {0899-9457, 1098-1098},
	shorttitle = {{\textless}span style="font-variant},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/ima.70013},
	doi = {10.1002/ima.70013},
	abstract = {ABSTRACT
            Many clinical procedures necessitate the integration of multi‐modality imaging data to facilitate more informed decision‐making. In practice, the cost of scanning and the potential health risks involved often make the scanning of multi‐modality images impractical. It is therefore important to explore the area of modality translation. In recent years, numerous studies have been conducted with the objective of developing methods for translating images between different modalities. Nevertheless, due to the substantial memory requirements and the difficulty in obtaining perfectly paired data, 3D volume modality translation remains a challenging topic. This research proposes a 3D generative adversarial network for the 3D CT‐MRI modality translation task. In order to leverage both low‐level features (pixel‐wise information) and high‐level features (overall image structure), our method introduces both convolutional and transformer structures. Furthermore, our method demonstrates robustness in the presence of imperfectly paired matched CT and MRI volumes from two medical datasets employed in the research. To validate the network performance, qualitative and quantitative comparisons and ablation studies were conducted. The results of the experiments demonstrate that the proposed framework can achieve good results in comparison to four other methods, with improvements of between 10\% and 20\% in four objective and one subjective evaluation metrics.},
	language = {en},
	number = {1},
	urldate = {2025-05-11},
	journal = {International Journal of Imaging Systems and Technology},
	author = {Ma, Ji and Xie, Yetao and Chen, Jinjin},
	month = jan,
	year = {2025},
	pages = {e70013},
	file = {PDF:C\:\\Users\\tobia\\Zotero\\storage\\Q328I5X9\\Ma et al. - 2025 - TransGAN  A Transformer‐ span style=font-variantsm.pdf:application/pdf},
}

@article{li_trans-cgan_2023,
	title = {Trans-{cGAN}: transformer-{Unet}-based generative adversarial networks for cross-modality magnetic resonance image synthesis},
	volume = {33},
	issn = {0960-3174, 1573-1375},
	shorttitle = {Trans-{cGAN}},
	url = {https://link.springer.com/10.1007/s11222-023-10282-8},
	doi = {10.1007/s11222-023-10282-8},
	language = {en},
	number = {5},
	urldate = {2025-05-11},
	journal = {Statistics and Computing},
	author = {Li, Yan and Han, Na and Qin, Yuxiang and Zhang, Jing and Su, Jinxia},
	month = oct,
	year = {2023},
	pages = {113},
	file = {PDF:C\:\\Users\\tobia\\Zotero\\storage\\ADSEQRIF\\Li et al. - 2023 - Trans-cGAN transformer-Unet-based generative adversarial networks for cross-modality magnetic reson.pdf:application/pdf},
}

@misc{yu_analysis_2024,
	title = {An {Analysis} for {Image}-to-{Image} {Translation} and {Style} {Transfer}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2408.06000},
	doi = {10.48550/ARXIV.2408.06000},
	abstract = {With the development of generative technologies in deep learning, a large number of image-to-image translation and style transfer models have emerged at an explosive rate in recent years. These two technologies have made significant progress and can generate realistic images. However, many communities tend to confuse the two, because both generate the desired image based on the input image and both cover the two definitions of content and style. In fact, there are indeed significant differences between the two, and there is currently a lack of clear explanations to distinguish the two technologies, which is not conducive to the advancement of technology. We hope to serve the entire community by introducing the differences and connections between image-to-image translation and style transfer. The entire discussion process involves the concepts, forms, training modes, evaluation processes, and visualization results of the two technologies. Finally, we conclude that image-to-image translation divides images by domain, and the types of images in the domain are limited, and the scope involved is small, but the conversion ability is strong and can achieve strong semantic changes. Style transfer divides image types by single image, and the scope involved is large, but the transfer ability is limited, and it transfers more texture and color of the image.},
	urldate = {2025-05-11},
	publisher = {arXiv},
	author = {Yu, Xiaoming and Tian, Jie and Hu, Zhenhua},
	year = {2024},
	note = {Version Number: 1},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, Image and Video Processing (eess.IV)},
	file = {PDF:C\:\\Users\\tobia\\Zotero\\storage\\99LA8FB7\\Yu et al. - 2024 - An Analysis for Image-to-Image Translation and Style Transfer.pdf:application/pdf},
}

@misc{arora_gans_2017,
	title = {Do {GANs} actually learn the distribution? {An} empirical study},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Do {GANs} actually learn the distribution?},
	url = {https://arxiv.org/abs/1706.08224},
	doi = {10.48550/ARXIV.1706.08224},
	abstract = {Do GANS (Generative Adversarial Nets) actually learn the target distribution? The foundational paper of (Goodfellow et al 2014) suggested they do, if they were given sufficiently large deep nets, sample size, and computation time. A recent theoretical analysis in Arora et al (to appear at ICML 2017) raised doubts whether the same holds when discriminator has finite size. It showed that the training objective can approach its optimum value even if the generated distribution has very low support ---in other words, the training objective is unable to prevent mode collapse. The current note reports experiments suggesting that such problems are not merely theoretical. It presents empirical evidence that well-known GANs approaches do learn distributions of fairly low support, and thus presumably are not learning the target distribution. The main technical contribution is a new proposed test, based upon the famous birthday paradox, for estimating the support size of the generated distribution.},
	urldate = {2025-05-11},
	publisher = {arXiv},
	author = {Arora, Sanjeev and Zhang, Yi},
	year = {2017},
	note = {Version Number: 2},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG)},
	file = {PDF:C\:\\Users\\tobia\\Zotero\\storage\\6FGWQYIX\\Arora und Zhang - 2017 - Do GANs actually learn the distribution An empirical study.pdf:application/pdf},
}

@misc{isola_image--image_2016,
	title = {Image-to-{Image} {Translation} with {Conditional} {Adversarial} {Networks}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1611.07004},
	doi = {10.48550/ARXIV.1611.07004},
	abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
	urldate = {2025-05-11},
	publisher = {arXiv},
	author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
	year = {2016},
	note = {Version Number: 3},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
	file = {PDF:C\:\\Users\\tobia\\Zotero\\storage\\GTSNGQMH\\Isola et al. - 2016 - Image-to-Image Translation with Conditional Adversarial Networks.pdf:application/pdf},
}

@article{pang_image--image_2022,
	title = {Image-to-{Image} {Translation}: {Methods} and {Applications}},
	volume = {24},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {1520-9210, 1941-0077},
	shorttitle = {Image-to-{Image} {Translation}},
	url = {https://ieeexplore.ieee.org/document/9528943/},
	doi = {10.1109/TMM.2021.3109419},
	urldate = {2025-05-11},
	journal = {IEEE Transactions on Multimedia},
	author = {Pang, Yingxue and Lin, Jianxin and Qin, Tao and Chen, Zhibo},
	year = {2022},
	pages = {3859--3881},
	file = {Eingereichte Version:C\:\\Users\\tobia\\Zotero\\storage\\233NY8PN\\Pang et al. - 2022 - Image-to-Image Translation Methods and Applications.pdf:application/pdf;Image-to-Image Translation - Methods and Applications:C\:\\Users\\tobia\\Zotero\\storage\\CXDA97TB\\Image-to-Image Translation - Methods and Applications.pdf:application/pdf},
}

@misc{noah_makow_wasserstein_2018,
	title = {Wasserstein {GANs} for {Image}-to-{Image} {Translation}},
	url = {https://cs230.stanford.edu/projects_spring_2018/reports/8289943.pdf},
	language = {Englisch},
	urldate = {2025-05-11},
	publisher = {Stanford},
	author = {{Noah Makow}},
	year = {2018},
	file = {PDF:C\:\\Users\\tobia\\Zotero\\storage\\4XBDTQHA\\Noah Makow - 2018 - Wasserstein GANs for Image-to-Image Translation.pdf:application/pdf},
}

@misc{gulrajani_improved_2017,
	title = {Improved {Training} of {Wasserstein} {GANs}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1704.00028},
	doi = {10.48550/ARXIV.1704.00028},
	abstract = {Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only low-quality samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models over discrete data. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.},
	urldate = {2025-05-11},
	publisher = {arXiv},
	author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron},
	year = {2017},
	note = {Version Number: 3},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML)},
	file = {PDF:C\:\\Users\\tobia\\Zotero\\storage\\VEA8BJ6Y\\Gulrajani et al. - 2017 - Improved Training of Wasserstein GANs.pdf:application/pdf},
}

@misc{liu_isb_2023,
	title = {I²{SB}: {Image}-to-{Image} {Schrödinger} {Bridge}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {I\${\textasciicircum}2\${SB}},
	url = {https://arxiv.org/abs/2302.05872},
	doi = {10.48550/ARXIV.2302.05872},
	abstract = {We propose Image-to-Image Schrödinger Bridge (I\${\textasciicircum}2\$SB), a new class of conditional diffusion models that directly learn the nonlinear diffusion processes between two given distributions. These diffusion bridges are particularly useful for image restoration, as the degraded images are structurally informative priors for reconstructing the clean images. I\${\textasciicircum}2\$SB belongs to a tractable class of Schrödinger bridge, the nonlinear extension to score-based models, whose marginal distributions can be computed analytically given boundary pairs. This results in a simulation-free framework for nonlinear diffusions, where the I\${\textasciicircum}2\$SB training becomes scalable by adopting practical techniques used in standard diffusion models. We validate I\${\textasciicircum}2\$SB in solving various image restoration tasks, including inpainting, super-resolution, deblurring, and JPEG restoration on ImageNet 256x256 and show that I\${\textasciicircum}2\$SB surpasses standard conditional diffusion models with more interpretable generative processes. Moreover, I\${\textasciicircum}2\$SB matches the performance of inverse methods that additionally require the knowledge of the corruption operators. Our work opens up new algorithmic opportunities for developing efficient nonlinear diffusion models on a large scale. scale. Project page and codes: https://i2sb.github.io/},
	urldate = {2025-05-11},
	publisher = {arXiv},
	author = {Liu, Guan-Horng and Vahdat, Arash and Huang, De-An and Theodorou, Evangelos A. and Nie, Weili and Anandkumar, Anima},
	year = {2023},
	note = {Version Number: 3},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML)},
	file = {PDF:C\:\\Users\\tobia\\Zotero\\storage\\DB5A5Y6X\\Liu et al. - 2023 - I²SB Image-to-Image Schrödinger Bridge.pdf:application/pdf},
}

@misc{glasmachers_limits_2017,
	title = {Limits of {End}-to-{End} {Learning}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1704.08305},
	doi = {10.48550/ARXIV.1704.08305},
	abstract = {End-to-end learning refers to training a possibly complex learning system by applying gradient-based learning to the system as a whole. End-to-end learning system is specifically designed so that all modules are differentiable. In effect, not only a central learning machine, but also all "peripheral" modules like representation learning and memory formation are covered by a holistic learning process. The power of end-to-end learning has been demonstrated on many tasks, like playing a whole array of Atari video games with a single architecture. While pushing for solutions to more challenging tasks, network architectures keep growing more and more complex. In this paper we ask the question whether and to what extent end-to-end learning is a future-proof technique in the sense of scaling to complex and diverse data processing architectures. We point out potential inefficiencies, and we argue in particular that end-to-end learning does not make optimal use of the modular design of present neural networks. Our surprisingly simple experiments demonstrate these inefficiencies, up to the complete breakdown of learning.},
	urldate = {2025-05-11},
	publisher = {arXiv},
	author = {Glasmachers, Tobias},
	year = {2017},
	note = {Version Number: 1},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML)},
	file = {PDF:C\:\\Users\\tobia\\Zotero\\storage\\2VCVIV9M\\Glasmachers - 2017 - Limits of End-to-End Learning.pdf:application/pdf},
}

@inproceedings{suri_grit_2024,
	address = {Waikoloa, HI, USA},
	title = {{GRIT}: {GAN} {Residuals} for {Paired} {Image}-to-{Image} {Translation}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {979-8-3503-1892-0},
	shorttitle = {{GRIT}},
	url = {https://ieeexplore.ieee.org/document/10483587/},
	doi = {10.1109/WACV57701.2024.00489},
	urldate = {2025-05-11},
	booktitle = {2024 {IEEE}/{CVF} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	publisher = {IEEE},
	author = {Suri, Saksham and Meshry, Moustafa and Davis, Larry S. and Shrivastava, Abhinav},
	month = jan,
	year = {2024},
	pages = {4953--4963},
	file = {PDF:C\:\\Users\\tobia\\Zotero\\storage\\MIFMV9BU\\Suri et al. - 2024 - GRIT GAN Residuals for Paired Image-to-Image Translation.pdf:application/pdf},
}

@misc{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1512.03385},
	doi = {10.48550/ARXIV.1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \&amp; COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2025-05-11},
	publisher = {arXiv},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2015},
	note = {Version Number: 1},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
	file = {PDF:C\:\\Users\\tobia\\Zotero\\storage\\RCZDVG7P\\He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf},
}

@misc{cai_cascade_2017,
	title = {Cascade {R}-{CNN}: {Delving} into {High} {Quality} {Object} {Detection}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Cascade {R}-{CNN}},
	url = {https://arxiv.org/abs/1712.00726},
	doi = {10.48550/ARXIV.1712.00726},
	abstract = {In object detection, an intersection over union (IoU) threshold is required to define positives and negatives. An object detector, trained with low IoU threshold, e.g. 0.5, usually produces noisy detections. However, detection performance tends to degrade with increasing the IoU thresholds. Two main factors are responsible for this: 1) overfitting during training, due to exponentially vanishing positive samples, and 2) inference-time mismatch between the IoUs for which the detector is optimal and those of the input hypotheses. A multi-stage object detection architecture, the Cascade R-CNN, is proposed to address these problems. It consists of a sequence of detectors trained with increasing IoU thresholds, to be sequentially more selective against close false positives. The detectors are trained stage by stage, leveraging the observation that the output of a detector is a good distribution for training the next higher quality detector. The resampling of progressively improved hypotheses guarantees that all detectors have a positive set of examples of equivalent size, reducing the overfitting problem. The same cascade procedure is applied at inference, enabling a closer match between the hypotheses and the detector quality of each stage. A simple implementation of the Cascade R-CNN is shown to surpass all single-model object detectors on the challenging COCO dataset. Experiments also show that the Cascade R-CNN is widely applicable across detector architectures, achieving consistent gains independently of the baseline detector strength. The code will be made available at https://github.com/zhaoweicai/cascade-rcnn.},
	urldate = {2025-05-11},
	publisher = {arXiv},
	author = {Cai, Zhaowei and Vasconcelos, Nuno},
	year = {2017},
	note = {Version Number: 1},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
	file = {PDF:C\:\\Users\\tobia\\Zotero\\storage\\4KRTTXNI\\Cai und Vasconcelos - 2017 - Cascade R-CNN Delving into High Quality Object Detection.pdf:application/pdf},
}

@article{ladwig_ai-guided_2024-1,
	title = {{AI}-{Guided} {Noise} {Reduction} for {Urban} {Geothermal} {Drilling}},
	volume = {2024},
	copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},
	url = {https://journals.hs-offenburg.de/index.php/urai/article/view/7},
	doi = {10.60643/URAI.V2024P85},
	abstract = {Urban geothermal energy production plays a critical role in achieving global climate objectives. However, drilling operations in densely populated areas generate significant noise pollution, posing challenges to community acceptance and regulatory compliance. This research presents an artificial intelligence-driven approach to dynamically reduce noise emissions during geothermal drilling. We integrate Deep Reinforcement Learning (DRL) with generative neural network models to provide real-time recommendations for optimal drilling parameters. Specifically, the Drill-LSTM model forecasts future machine states, while the Sound-GAN framework predicts sound propagation based on varying operational conditions. These models feed into a DRL-Agent that learns to balance drilling efficiency with noise minimization. Additionally, an interactive assistance system GUI presents predictions, forecasts, and recommendations to human operators, facilitating informed decision-making. Our system demonstrates significant potential in reducing noise levels, enhancing operational efficiency, and fostering greater acceptance of urban geothermal projects. Future work will focus on refining the models and validating the system in real-world drilling scenarios.},
	language = {en},
	number = {Proceedings of the Upper-Rhine Artificial Intelligence Symposium, Proceedings of the Upper-Rhine Artificial Intelligence Symposium 2024},
	urldate = {2025-05-11},
	journal = {Proceedings of the Upper-Rhine Artificial Intelligence Symposium},
	author = {Ladwig, Daniel and Spitznagel, Martin and Vaillant, Jan and Dorer, Klaus and Keuper, Janis},
	month = oct,
	year = {2024},
	note = {Artwork Size: 4MB
Publisher: Proceedings of the Upper-Rhine Artificial Intelligence Symposium},
	keywords = {AI-Assisted Control, Deep Reinforcement Learning, Generative Models, Geothermal Drilling, Noise Reduction},
	pages = {85--94},
	file = {PDF:C\:\\Users\\tobia\\Zotero\\storage\\RMHZAJLV\\Ladwig et al. - 2024 - AI-Guided Noise Reduction for Urban Geothermal Drilling.pdf:application/pdf},
}

@misc{spitznagel_urban_2024-1,
	title = {Urban {Sound} {Propagation}: a {Benchmark} for 1-{Step} {Generative} {Modeling} of {Complex} {Physical} {Systems}},
	copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},
	shorttitle = {Urban {Sound} {Propagation}},
	url = {https://arxiv.org/abs/2403.10904},
	doi = {10.48550/ARXIV.2403.10904},
	abstract = {Data-driven modeling of complex physical systems is receiving a growing amount of attention in the simulation and machine learning communities. Since most physical simulations are based on compute-intensive, iterative implementations of differential equation systems, a (partial) replacement with learned, 1-step inference models has the potential for significant speedups in a wide range of application areas. In this context, we present a novel benchmark for the evaluation of 1-step generative learning models in terms of speed and physical correctness. Our Urban Sound Propagation benchmark is based on the physically complex and practically relevant, yet intuitively easy to grasp task of modeling the 2d propagation of waves from a sound source in an urban environment. We provide a dataset with 100k samples, where each sample consists of pairs of real 2d building maps drawn from OpenStreetmap, a parameterized sound source, and a simulated ground truth sound propagation for the given scene. The dataset provides four different simulation tasks with increasing complexity regarding reflection, diffraction and source variance. A first baseline evaluation of common generative U-Net, GAN and Diffusion models shows, that while these models are very well capable of modeling sound propagations in simple cases, the approximation of sub-systems represented by higher order equations systematically fails. Information about the dataset, download instructions and source codes are provided on our website: https://www.urban-sound-data.org.},
	urldate = {2025-05-11},
	publisher = {arXiv},
	author = {Spitznagel, Martin and Keuper, Janis},
	year = {2024},
	note = {Version Number: 2},
	keywords = {Audio and Speech Processing (eess.AS), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, Sound (cs.SD)},
	file = {PDF:C\:\\Users\\tobia\\Zotero\\storage\\KS3S3R84\\Spitznagel und Keuper - 2024 - Urban Sound Propagation a Benchmark for 1-Step Generative Modeling of Complex Physical Systems.pdf:application/pdf},
}

@misc{spitznagel_physicsgen_2025,
	title = {{PhysicsGen}: {Can} {Generative} {Models} {Learn} from {Images} to {Predict} {Complex} {Physical} {Relations}?},
	copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},
	shorttitle = {{PhysicsGen}},
	url = {https://arxiv.org/abs/2503.05333},
	doi = {10.48550/ARXIV.2503.05333},
	abstract = {The image-to-image translation abilities of generative learning models have recently made significant progress in the estimation of complex (steered) mappings between image distributions. While appearance based tasks like image in-painting or style transfer have been studied at length, we propose to investigate the potential of generative models in the context of physical simulations. Providing a dataset of 300k image-pairs and baseline evaluations for three different physical simulation tasks, we propose a benchmark to investigate the following research questions: i) are generative models able to learn complex physical relations from input-output image pairs? ii) what speedups can be achieved by replacing differential equation based simulations? While baseline evaluations of different current models show the potential for high speedups (ii), these results also show strong limitations toward the physical correctness (i). This underlines the need for new methods to enforce physical correctness. Data, baseline models and evaluation code http://www.physics-gen.org.},
	urldate = {2025-05-11},
	publisher = {arXiv},
	author = {Spitznagel, Martin and Vaillant, Jan and Keuper, Janis},
	year = {2025},
	note = {Version Number: 1},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
	file = {PDF:C\:\\Users\\tobia\\Zotero\\storage\\7HHS52DL\\Spitznagel et al. - 2025 - PhysicsGen Can Generative Models Learn from Images to Predict Complex Physical Relations.pdf:application/pdf},
}

@misc{achim_eckerle_evaluierung_2025-1,
	title = {Evaluierung der physikalischen {Korrektheit} von {Normalizing} {Flows} bei {Bild}-zu-{Bild} {Transformationen} in {Schallausbreitungssimulationen}},
	language = {Deutsch},
	author = {{Achim Eckerle}},
	year = {2025},
	file = {PDF:C\:\\Users\\tobia\\Zotero\\storage\\UTLELC2K\\Achim Eckerle - 2025 - Evaluierung der physikalischen Korrektheit von Normalizing Flows bei Bild-zu-Bild Transformationen i.pdf:application/pdf},
}

@incollection{hu_deep_2020,
	address = {Cham},
	title = {Deep {Image} {Translation} for {Enhancing} {Simulated} {Ultrasound} {Images}},
	volume = {12437},
	isbn = {978-3-030-60333-5 978-3-030-60334-2},
	url = {https://link.springer.com/10.1007/978-3-030-60334-2_9},
	language = {en},
	urldate = {2025-05-11},
	booktitle = {Medical {Ultrasound}, and {Preterm}, {Perinatal} and {Paediatric} {Image} {Analysis}},
	publisher = {Springer International Publishing},
	author = {Zhang, Lin and Portenier, Tiziano and Paulus, Christoph and Goksel, Orcun},
	editor = {Hu, Yipeng and Licandro, Roxane and Noble, J. Alison and Hutter, Jana and Aylward, Stephen and Melbourne, Andrew and Abaci Turk, Esra and Torrents Barrena, Jordina},
	year = {2020},
	doi = {10.1007/978-3-030-60334-2_9},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {85--94},
	file = {PDF:C\:\\Users\\tobia\\Zotero\\storage\\MLBYVMIA\\Zhang et al. - 2020 - Deep Image Translation for Enhancing Simulated Ultrasound Images.pdf:application/pdf},
}

@article{armanious_medgan_2020,
	title = {{MedGAN}: {Medical} image translation using {GANs}},
	volume = {79},
	issn = {08956111},
	shorttitle = {{MedGAN}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0895611119300990},
	doi = {10.1016/j.compmedimag.2019.101684},
	language = {en},
	urldate = {2025-05-11},
	journal = {Computerized Medical Imaging and Graphics},
	author = {Armanious, Karim and Jiang, Chenming and Fischer, Marc and Küstner, Thomas and Hepp, Tobias and Nikolaou, Konstantin and Gatidis, Sergios and Yang, Bin},
	month = jan,
	year = {2020},
	pages = {101684},
	file = {Eingereichte Version:C\:\\Users\\tobia\\Zotero\\storage\\V7G53YFD\\Armanious et al. - 2020 - MedGAN Medical image translation using GANs.pdf:application/pdf},
}
