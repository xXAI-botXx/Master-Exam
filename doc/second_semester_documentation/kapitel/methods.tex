\chapter{Methods}
\label{cha:methods}

	\section{Data}
	\label{sec:method-data}
	We run all experiments on the \textbf{PhysicsGen}-Benchmark \cite{martin_spitznagel_physicsgen_2025}, which provides 2D wave-propagation simulations across urban environments. The dataset contains approximately \textbf{20.000} training, \textbf{2.000} test, and \textbf{1.200} evaluation samples with the official split. Each sample is available in four variations that differ in physical complexity:
	\begin{itemize}[itemsep=0pt, topsep=0pt, parsep=0pt, partopsep=0pt]
		\item \textbf{Base-Propagation}
		\item \textbf{Base-Propagation + Reflection}
		\item \textbf{Base-Propagation + Diffraction}
		\item \textbf{Base-Propagation + Reflection + Diffraction}
	\end{itemize} 
	In this study, we focus on the \textbf{reflection} variation, as it represents a balanced level of complexity \cite{martin_spitznagel_physicsgen_2025} while still exposing the performance limitations of existing learning-based approaches, as noted in the original PhysicsGen publication.\\
	Each sample consists of a 2D gray-image representing the physical scene - specifically, the image indicates the locations of buildings, which are the main obstacles for sound propagation in urban areas â€“ along with the corresponding simulated wave-intensity distribution from a fixed transmitter. \\
	All datapoints are collected from 3 cities in Germany \cite{martin_spitznagel_physicsgen_2025} and the ground-truth is created via NoiseModelling, a physical simulation software \cite{noisemodelling}.\\
	For evaluation, we make a distinction between \textbf{line-of-sight (LoS)} regions and \textbf{non-line-of-sight (NLoS)} regions, where NLoS areas exhibit stronger interactions with environmental structures. Modeling these physically complex regions is a primary focus of this work.
	
	
	\clearpage
	
	\section{Preprocessing}
	\label{sec:method-preprocessing}
	All samples are rescaled to a fixed spatial resolution of 256x256 before being fed into the neural networks. Intensities are normalized to [0.0, 1.0].\\
	In experiments that include \textbf{ray-tracing features}, we compute an additional channel or one channel for every single ray (36 rays result in 36 additional channels) using our custom 2D ray-tracing implementation, which estimates the geometric reflection paths between the transmitter and each pixel. This process can be optimized by pre-computing and saving the ray-traces, then our deep-learning pipeline will automatically load the traces from the local file system.\\
	No data augmentation is applied in order to ensure comparability with prior work but could be tested in future works.\\
	For experiments involving \textbf{pre-masking} or \textbf{post-masking}, the loss is multiplied by a binary swapping mask to restrict learning to a selected subset and thereby reduce task difficulty. The only difference between the two settings is the time at which the mask is applied during training.
	
	
	
	
	\section{Deep Learning Models}
	\label{sec:method-deep-learning-models}
	\subsection{Pix2Pix Baseline}
	We use a standard Pix2Pix conditional generative architecture as our primary baseline, due to its fast training and accurate result capabilities shown in the Physgen Paper \cite{martin_spitznagel_physicsgen_2025}. The generator is a U-Net with skip connections, and the discriminator follows a PatchGAN design.\\
	We introduce the following modifications:
	\begin{itemize}[itemsep=0pt, topsep=0pt, parsep=0pt, partopsep=0pt]
		\item optional pre-masking and post-masking
		\item an additional physical-information channel (ray-tracing) without a specific fusion component
		\item a combined weighted loss *(add formular with weights/losses or add loss-component names?)
	\end{itemize} 
	
	\clearpage
	
	\subsection{Pix2Pix Residual Design}
	We additionally introduced the Pix2Pix Residual Design Model, which we also use here and which we developed in our previous work. This model consists of two Pix2Pix networks and a CNN fusion head (optionally replaceable by a simple formula-based calculation) that combines the predictions of both networks to the final prediction. The task is divided into two subtasks: baseline propagation and the complex-only component (reflections). The target for the complex-only component is obtained by subtracting the baseline sound propagation from the reflection sound propagation, since the reflection inherently includes the baseline.\\
	Although this model achieved poor accuracies \cite{first_semester_report}, our previous investigation showed that this was caused by the complex-only prediction, which frequently converged to a local minimum (a nearly uniform gray image). This occurs because the target values are very close to each other *[show histogram]. Prior work introduced a loss-weighting mechanism based on the histogram, as well as additional variance and min/max losses, to mitigate this issue and improve performance.
	
	
	\subsection{Cosmos Predict 2 (Video2World)}
	We further evaluate the \textbf{Cosmos Predict 2 Video2World} model from NVIDIA, a large transformer-based world-model originally designed for multimodal environment prediction. \\
	In our setup, the model receives the PhysicsGen scene representation as input and predicts the resulting wave-intensity map in a single forward pass. The model is used with its default architecture but adapted to accept PhysicsGen-style 2D grids.
	
	
	
	\section{Ray-Tracing Baseline}
	\label{sec:method-rays}
	To provide a physically interpretable reference for the model, we implement a lightweight 2D ray-tracing simulation. The algorithm computes x-order reflection paths without calculating the ray energies, in order to keep the complexity and computational cost low. Although much simpler than a full wave simulation, this baseline may still help to predict reflection patterns.\\
	We used third-order reflections with 36 and 360 rays as one and as multiple channels in our experiments.
	
	\clearpage
	
	\section{Training Procedure}
	\label{sec:method-training}
	All models are trained using the Adam optimizer with a learning rate of \(5 \times 10^{-5}\), \(\beta_1 = 0.5\), and \(\beta_2 = 0.999\).\\
	The batch size is set to 64 or 128 unless GPU memory constraints require reduction.
	Training is performed for 50 to 100 epochs, with early stopping based on validation loss. These numbers are taken from \citetitle{martin_spitznagel_physicsgen_2025}.\\
	The combined weighted loss is most likely used for the complex only model and else a L1-loss is used for Pix2Pix with a small adversarial loss for sharper predictions. Cosmos Predict 2 uses its native reconstruction loss.\\
	All experiments are based on PyTorch 2.1, CUDA 12.2 \& 13.0, and Python 3.12. While the evaluation script (again officially taken from PhysicsGen \cite{martin_spitznagel_physicsgen_2025}) uses Python 3.8 in order to fulfill all requirements.
	
	
	\section{Hardware}
	\label{sec:method-hardware}
	Experiments are conducted using two hardware configurations:
	\begin{itemize}[itemsep=0pt, topsep=0pt, parsep=0pt, partopsep=0pt]
		\item a workstation with \textbf{one NVIDIA RTX 4090 (24 GB)}
		\item a server with \textbf{four NVIDIA A100 GPUs (40 GB each)}
	\end{itemize} 
	The multi-GPU setup is required for training large transformer-based models such as Cosmos Predict 2.
	
	
	\section{Evaluation Metrics}
	\label{sec:method-eval}
	We evaluate all methods using:
	\begin{itemize}[itemsep=0pt, topsep=0pt, parsep=0pt, partopsep=0pt]
		\item Mean Absolute Error (MAE)
		\item Weighted Mean Absolute Percentage Error (wMAPE)
	\end{itemize} 
	Metrics are computed separately for:
	\begin{itemize}[itemsep=0pt, topsep=0pt, parsep=0pt, partopsep=0pt]
		\item LoS regions
		\item NLoS regions
	\end{itemize}
	This separation allows us to specifically measure performance in physically complex areas, where reflections, scattering, and occlusions are prevalent.
	For comparability, we follow the evaluation protocol of \citeauthor{martin_spitznagel_physicsgen_2025}, using identical region definitions and aggregation schemes.
	
	%and \cite{eckerle2025realtimepredictionurbansound}
	
	
	


